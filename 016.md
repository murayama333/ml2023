# 最小二乗法
* データセットに最も適合する線形回関数を見つける手法
* 予測値と実測値の誤差（残差）の二乗和を最小化する
* たとえばモデルを $f(x) = a x +b$ とすると次のようになる

$$
E(a, b) = \sum_{i=1}^n (y_i - a x_i - b )^2
$$

> ここでx, yは定数である点に注意します。Eはa, bの関数です。

## まとめ

$$
a = \frac{\bar{xy} - \bar{x} \bar{y} }{\bar{x^2} -\bar{x}^2} = \frac{\sigma_{xy}}{\sigma_x^2}
$$

$$
b = -a\bar{x} + \bar{y}
$$

## 参考：解法

* a, bで偏微分した結果が0となる点を求める

$$
\frac{\partial E(a, b) }{ \partial a} = -2\sum_{i=1}^nx_i (y_i - a x_i - b) = 0
$$

$$
\frac{\partial E(a, b) }{ \partial b} = 2\sum_{i=1}^n (y_i - a x_i - b) = 0
$$

* 次のように整理できる

$$
a\sum_{i=1}^n x_i^2 + b \sum_{i=1}^n x_i = \sum_{i=1}^n x_i y_i \tag{1}
$$

$$
a\sum_{i=1}^n x_i + b n = \sum_{i=1}^n y_i \tag{2}
$$

* (2)の両辺をnで割る

$$
a\frac{1}{n}\sum_{i=1}^n x_i + b = \frac{1}{n}\sum_{i=1}^n y_i
$$

$$
a\bar{x} + b = \bar{y}
$$

$$
b = -a\bar{x} + \bar{y}
$$

* これを(1)に代入する

$$
a\sum_{i=1}^n x_i^2 + (-a\bar{x} + \bar{y}) \sum_{i=1}^n x_i = \sum_{i=1}^n x_i y_i
$$

$$
a\sum_{i=1}^n x_i^2 -a\bar{x}\sum_{i=1}^n x_i + \bar{y}\sum_{i=1}^n x_i  = \sum_{i=1}^n x_i y_i
$$

$$
a(\sum_{i=1}^n x_i^2 -\bar{x}\sum_{i=1}^n x_i) + \bar{y}\sum_{i=1}^n x_i  = \sum_{i=1}^n x_i y_i \tag{3}
$$

* (3)の両辺をnで割る

$$
a(\frac{1}{n} \sum_{i=1}^n x_i^2 -\bar{x} \frac{1}{n} \sum_{i=1}^n x_i) + \bar{y}\frac{1}{n}\sum_{i=1}^n x_i  = \frac{1}{n}\sum_{i=1}^n x_i y_i
$$

$$
a(\bar{x^2} -\bar{x}^2) + \bar{x} \bar{y} = \bar{xy}
$$

$$
a = \frac{\bar{xy} - \bar{x} \bar{y} }{\bar{x^2} -\bar{x}^2}
$$

* 分散と共分散を使って以下のように表現できる

$$
a = \frac{\sigma_{xy}}{\sigma_x^2}
$$

---

## サンプルコード

```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import pandas as pd

iris = load_iris()
iris_data = pd.DataFrame(iris.data, columns=iris.feature_names)
target = iris_data["sepal length (cm)"]
data = iris_data[["sepal width (cm)",	"petal length (cm)",	"petal width (cm)"]]

x_train, x_test, y_train, y_test = train_test_split(data, target)

model = LinearRegression()
model.fit(x_train, y_train)
print("score:", model.score(x_test, y_test))
print("intercept:", model.intercept_)
print("coef:",  model.coef_)
```

### 実行結果

```
score: 0.8754951593882535
intercept: 1.8640508817830117
coef: [ 0.66255093  0.66948033 -0.46952699]
```

---

## 参考：AIによる考察

残差の二乗和の最小化: 最小二乗法は、予測値と実測値の差（残差）の二乗和を最小化することにより、データセットに最も適合する線を見つける手法です。この方法は、誤差の大きさを強調し、正負の影響を相殺しないために二乗和を用いています。

外れ値への敏感性: 最小二乗法は外れ値に非常に敏感であり、これらの値は残差の二乗和に大きく影響し、モデルの精度を低下させる可能性があります。データの前処理で外れ値を取り除くか、別の手法を検討することが重要です。

非線形関係への対応: 最小二乗法は線形モデルに最適ですが、非線形関係を持つデータには適していません。非線形データに対応するためには、多項式回帰や他の非線形モデルを使用することが望ましいです。
